{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1cf375b-87a5-4d4e-9b3c-d9d0d500c948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import shutil\n",
    "import os\n",
    "import random\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52efe55e-4539-4ded-810a-2e0c8f0c5f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tmx(path):\n",
    "    tree = ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "    result = []\n",
    "    \n",
    "    for tu in root.findall(\".//tu\"):\n",
    "        content = []\n",
    "\n",
    "        tuvs = tu.findall(\"tuv\")\n",
    "        if len(tuvs) != 2:\n",
    "            print(f\"Warning: {len(tuvs)} tuvs\")\n",
    "            \n",
    "        for tuv in tu.findall(\"tuv\"):\n",
    "            lang = tuv.attrib.get(\"{http://www.w3.org/XML/1998/namespace}lang\")\n",
    "            seg = tuv.find(\"seg\")\n",
    "            if seg is not None and seg.text is not None:\n",
    "                content.append({\"lang\": lang, \"text\": seg.text})\n",
    "        \n",
    "        if len(content) == 2:\n",
    "            result.append(content)\n",
    "        else:\n",
    "            pass\n",
    "            # print(f\"Skip file: {path}. \\nUnexpected content size: {len(content)}\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1227dfbf-b5b4-4014-9d8f-f0c4ce39ef6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_documents_from_xml(root_dir):\n",
    "    documents = []\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        filtered_files = [f for f in filenames if f.lower().endswith('.tmx')]\n",
    "        if not filtered_files:\n",
    "            continue\n",
    "\n",
    "        rel_path = os.path.relpath(dirpath, root_dir)\n",
    "        if rel_path == \".\":\n",
    "            folder_hierarchy = []\n",
    "        else:\n",
    "            folder_hierarchy = rel_path.split(os.sep)\n",
    "\n",
    "        for xml_file in filtered_files:\n",
    "            file_path = os.path.join(dirpath, xml_file)\n",
    "            try:\n",
    "                content_list = parse_tmx(file_path)\n",
    "                for content in content_list:\n",
    "                    try:\n",
    "                        doc = {\n",
    "                            \"page_content\": content[0][\"text\"], \n",
    "                            \"metadata\": {\n",
    "                                \"lang\": content[0][\"lang\"],\n",
    "                                \"file_path\": file_path,\n",
    "                                \"translation_lang\": content[1][\"lang\"],\n",
    "                                \"translation_text\": content[1][\"text\"],\n",
    "                                \"lang_dir\": folder_hierarchy[0],\n",
    "                                \"folders\": \",\".join(folder_hierarchy[1:])\n",
    "                            }}\n",
    "                        documents.append(doc)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Failed to create doc\\nFile: {file_path}\\n with content\\n: {content}\")\n",
    "                        print(f\"Error: {e}\\n\\n\")\n",
    "\n",
    "            except ET.ParseError as pe:\n",
    "                print(f\"Failed to process '{file_path}': {pe}\\n\")\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43dda900-7d9b-432b-ab14-9c88f0852168",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = create_documents_from_xml(\"documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2f8c2c-3a1a-46f0-8ed7-94d12df0369f",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_filtered = [d for d in doc_list if len(d[\"page_content\"]) < 4]\n",
    "print([(d['page_content'], d['metadata']['translation_text']) for d in docs_filtered])\n",
    "\n",
    "print(len(docs_filtered))\n",
    "if len(docs_filtered) > 0:\n",
    "    doc = random.sample(docs_filtered, 1)[0]\n",
    "    print(f\"{doc['page_content']}\\n{doc['metadata']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c39e71f1-e4f1-4e01-b16c-71c4f7439358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 231377 documents.\n"
     ]
    }
   ],
   "source": [
    "documents = [\n",
    "    Document(\n",
    "        page_content=doc_item[\"page_content\"], \n",
    "        metadata={\n",
    "            \"lang\": doc_item[\"metadata\"][\"lang\"],\n",
    "            \"file_path\": doc_item[\"metadata\"][\"file_path\"],\n",
    "            \"translation_lang\": doc_item[\"metadata\"][\"translation_lang\"],\n",
    "            \"translation_text\": doc_item[\"metadata\"][\"translation_text\"],\n",
    "            \"lang_dir\": doc_item[\"metadata\"][\"lang_dir\"],\n",
    "            \"folders\": doc_item[\"metadata\"][\"folders\"]\n",
    "        }) for doc_item in doc_list]\n",
    "\n",
    "print(f\"Created {len(documents)} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e5c23f-1d21-4820-afea-03b356ac76a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in documents[200_000:200_001]:\n",
    "    print(d.page_content)\n",
    "    print(d.metadata)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb55fe9d-ae72-457d-9516-a6aa603757f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From all 231377 documents 232588 chunks created.\n"
     ]
    }
   ],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(documents)\n",
    "print(f\"From all {len(documents)} documents {len(chunks)} chunks created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1b76b65-cf7a-49f6-8580-0af7149dc644",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "def get_batches(chunks, batch_size=512):\n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        yield chunks[i:i+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8600ca06-0322-4096-b45b-5128ac1ebb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In memory vector store for experiments, limited chunks[:20_000]\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"in_memory_docs\",  \n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=None\n",
    ")\n",
    "\n",
    "if vectorstore._collection.count() == 0:\n",
    "    for batch in tqdm(get_batches(chunks[:20_000]), desc=\"Adding documents\", unit=\"batch\"):\n",
    "        vectorstore.add_documents(batch)\n",
    "\n",
    "total_docs = vectorstore._collection.count()\n",
    "print(f\"Created In-Memory vectorstore. Found {total_docs} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09565c5b-3479-43cb-addb-94afb1e89345",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding documents: 455batch [24:15,  3.20s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 232588 documents.\n",
      "Found 232588 documents in Chroma vectorstore `translation_db`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Persistent vector db, it takes ~ 30 minutes (on my MacBook Pro M4) to add 232588 chunks\n",
    "\n",
    "db_name = 'translation_db'\n",
    "db_path = Path(db_name)\n",
    "\n",
    "vectorstore = Chroma(persist_directory=db_name, embedding_function=embeddings)\n",
    "\n",
    "total_docs = vectorstore._collection.count()\n",
    "if total_docs == 0:\n",
    "    for batch in tqdm(get_batches(chunks), desc=\"Adding documents\", unit=\"batch\"):\n",
    "        vectorstore.add_documents(batch)\n",
    "    print(f\"Added {vectorstore._collection.count()} documents.\")\n",
    "\n",
    "total_docs = vectorstore._collection.count()\n",
    "print(f\"Found {total_docs} documents in Chroma vectorstore `{db_name}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fc371f-4124-4519-8bb0-0ac306fcc2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After destroying it is required to restart the Kernel to create a new db with the same path\n",
    "def destroy_persistent_vector_store():\n",
    "    shutil.rmtree(db_name, ignore_errors=True)\n",
    "destroy_persistent_vector_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1633eece-2a28-4298-be01-bc8110d6f5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metadata_to_string(metadata):\n",
    "    s = \"\"\n",
    "    for key in sorted(metadata.keys()):\n",
    "        s += f\"{key}: {metadata[key]}\\n\"\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c56c5ab-3f0f-4f37-8850-72f0e6c11b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Score=0.170698]\n",
      "Уставом общества может быть запрещено отчуждение долей третьим лицам.\n",
      "file_path: documents/RU-DE/special language/official-business/Economics/Инвестиции в Смоленской области ОД  .tmx\n",
      "folders: special language,official-business,Economics\n",
      "lang: ru\n",
      "lang_dir: RU-DE\n",
      "translation_lang: de\n",
      "translation_text: Das Recht zur Übertragung (einschließlich des Verkaufs) von Anteilen an dritte Personen kann durch die Satzung ausgeschlossen werden.\n",
      "\n",
      "\n",
      "\n",
      "[Score=0.171978]\n",
      "Такому должностному лицу (лицам) может быть предписано частично или полностью возместить потери Организации Объединенных Наций.\n",
      "file_path: documents/RU-DE/special language/official-business/UN/Финансовые положения и правила Организации Объединенных Наций ОД С.tmx\n",
      "folders: special language,official-business,UN\n",
      "lang: ru\n",
      "lang_dir: RU-DE\n",
      "translation_lang: de\n",
      "translation_text: Ist dies der Fall, so kann von dem Betroffenen verlangt werden, den Vereinten Nationen den Verlust teilweise oder in voller Höhe zu erstatten.\n",
      "\n",
      "\n",
      "\n",
      "[Score=0.171978]\n",
      "Такому должностному лицу (лицам) может быть предписано частично или полностью возместить потери Организации Объединенных Наций.\n",
      "file_path: documents/RU-DE/special language/official-business/UN/Финансовые положения и правила Организации Объединенных Наций ОД С.tmx\n",
      "folders: special language,official-business,UN\n",
      "lang: ru\n",
      "lang_dir: RU-DE\n",
      "translation_lang: de\n",
      "translation_text: Ist dies der Fall, so kann von dem Betroffenen verlangt werden, den Vereinten Nationen den Verlust teilweise oder in voller Höhe zu erstatten.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Документы должны быть засекречены третьими лицами\"\n",
    "results = vectorstore.similarity_search_with_score(\n",
    "    query=query, \n",
    "    k=3,\n",
    "    filter={\"lang\": \"ru\"}\n",
    ")\n",
    "\n",
    "for doc, score in results:\n",
    "    print(f\"\"\"[Score={score:3f}]\\n{doc.page_content}\\n{metadata_to_string(doc.metadata)}\\n\\n\"\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c73fab-5e8f-4be0-a9df-8a378ca3163f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
